{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be89c0a",
   "metadata": {},
   "source": [
    "# Zillow Clustering Project\n",
    "Sophia Stewart<br>\n",
    "Stephanie Jones<br>\n",
    "Codeup | Data Science, Hopper Cohort<br>\n",
    "Monday, January 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3fca5",
   "metadata": {},
   "source": [
    "# About the Project\n",
    "#### Goal\n",
    "Identify drivers of error in predicting home value for single family properties<br>\n",
    "#### Why?\n",
    "We want to improve our zestimate home value predictions so that we can better serve those who purchase and sell homes<br>\n",
    "\n",
    "# Data Dictionary\n",
    "### DataFrames\n",
    "| DFs | Meaning |\n",
    "| :-------- | -------: |\n",
    "| zillow   | Full, original dataframe retrieved from the zillow mySQL database |\n",
    "| train    | Sample (56%) of zillow used for exploring data and fitting/training models|\n",
    "| validate | Sample (24%) of zillow used to evaluate multiple models |\n",
    "| test     | Sample (20%) of zillow used to evaluate the best model |\n",
    "\n",
    "### Variables\n",
    "| Target | Meaning |\n",
    "| :-------- | -------: |\n",
    "| logerror | Our target variable; the Zestimate error which we want to minimize |\n",
    "\n",
    "\n",
    "| Variables | Meaning |\n",
    "| :-------- | -------: |\n",
    "| tax_value | The property's tax assessed value |\n",
    "| beds     | Number of bedrooms |\n",
    "| baths    | Number of bathrooms, including fractional bathrooms |\n",
    "| fullbaths | Number of full bathrooms |\n",
    "| latitude | The property's latitude |\n",
    "| longitude | The property's longitude |\n",
    "| sq_ft    | Calculated total finished living area |\n",
    "| yearbuilt | The year the property was built |\n",
    "| age      | The age of the property |\n",
    "| transactiondate | The date the property was sold |\n",
    "\n",
    "<!-- | Clustering | Meaning |\n",
    "| :-------- | -------: |\n",
    "| beds_scaled | Standard-scaled `beds` |\n",
    "| baths_scaled | Standard-scaled `baths` |\n",
    "| sq_ft_scaled | Standard-scaled `sq_ft` |\n",
    "\n",
    "\n",
    "| Modeling | Meaning |\n",
    "| :-------- | -------: |\n",
    "| x_train  | `train`, with scaled `tax_value`, `age`, `sqft` columns |\n",
    "| y_train  | `train`, but only the target |\n",
    "| x_validate | `validate`, with scaled `tax_value`, `age`, `sqft` columns |\n",
    "| y_validate | `validate`, but only the target |\n",
    "| x_test   | `test`, with scaled `tax_value`, `age`, `sqft` columns |\n",
    "| y_test   | `test`, but only the target | -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa65815",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "High `logerror` threatens our credibility as a primary source of home valuation predictions in the real estate market. We want to find the drivers of Zestimate prediction `logerror` to provide more accurate home valuations. \n",
    "<br><br>\n",
    "To do this, we will use clustering to identify patterns in our 2017 single-unit property data and use those clusters to build a model which will be used for predicting logerror. If we can predict `logerror`, we can use those predictions to make more accurate predictions of home values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee993f5",
   "metadata": {},
   "source": [
    "# Step 1 | Acquire and Wrangle\n",
    "<hr>\n",
    "\n",
    "In our `wrangle.py` module you will find the following functions:\n",
    "- `acquire_zillow()` acquires zillow data from a csv file or from a sequel query (see query inside of function within module)\n",
    "- `clean_zillow()` cleans the acquired data\n",
    "    - filter out non-single unit properties using `propertylandusetypeid`, `beds`, `baths`, and `sqft`\n",
    "    - drop null rows and columns with > 50% missing values\n",
    "    - create `age` column from `yearbuilt`\n",
    "    - drop any remaining null values\n",
    "    - correct dtypes for int values\n",
    "    - drop `propertylandusetypeid`, `transactiondate`, `yearbuilt`, `unitcnt`\n",
    "    - remove outliers\n",
    "- `split_zillow(df)` split data into train, validate, and test dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d21f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021b21ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sqft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9bacf87659cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrangle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_zillow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_zillow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_zillow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train Shape: {train.shape}\\nValidate Shape: {validate.shape}\\nTest Shape: {test.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/zillow-clustering-project/wrangle.py\u001b[0m in \u001b[0;36mclean_zillow\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#1) filter single units by properylandusetype, bath, bed, and sqft count, and unit count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropertylandusetypeid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m261\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m262\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m263\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m264\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m266\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m268\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m273\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m276\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m279\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaths\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeds\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqft\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#2) dropping null rows and columns with > n% of values missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sqft'"
     ]
    }
   ],
   "source": [
    "import wrangle as w\n",
    "\n",
    "train, validate, test = w.split_zillow(w.clean_zillow(w.acquire_zillow()))\n",
    "\n",
    "print(f'Train Shape: {train.shape}\\nValidate Shape: {validate.shape}\\nTest Shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e17e8",
   "metadata": {},
   "source": [
    "# Step 2 | Explore\n",
    "<hr>\n",
    "\n",
    "In our `explore.py` module you will find the following functions for exploration\n",
    "- Univaratiate\n",
    "    - `visualize_outliers():` two rows of boxplots, data before and after outliers removed\n",
    "    - `var_distributions():` plots histograms of each of our variables\n",
    "- Bivariate\n",
    "- Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import explore_stewart as e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e666f",
   "metadata": {},
   "source": [
    "## Univariate Exploration\n",
    "##### Are there any extreme values in our outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a87e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.visualize_outliers(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad8f25",
   "metadata": {},
   "source": [
    ">#### We can now actually see the IQR for `bath`, `sqft`, `fullbaths`, `tax_value` and `logerror`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff8a6c",
   "metadata": {},
   "source": [
    "##### What are the distributions for each of our variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8cd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.var_distributions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668263b2",
   "metadata": {},
   "source": [
    ">#### Normal\n",
    "Our target variable, `logerror` is ~normally distrubuted.<br>`longitude` is also somewhat normally distrubuted. \n",
    "\n",
    ">#### Slightly Right Skewed \n",
    "`baths`, `fullbaths`, <br>`sqft`, <br>`latitude`, <br>`tax_value`, and <br>`age` \n",
    "\n",
    ">#### Slightly Left Skewed\n",
    "`beds`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4aae5d",
   "metadata": {},
   "source": [
    "## Multivariate Exploration\n",
    "##### How does each independent variable interact with the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "target, ind_vars = e.plot_vars(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a5187",
   "metadata": {},
   "source": [
    "#### `logerror` is our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(train.columns, columns = ['target'])[7:8]\n",
    "target.reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13821f12",
   "metadata": {},
   "source": [
    "#### These are the independent variables we will be exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fba9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(train.columns, columns = ['independent_vars']).drop([7])\n",
    "ind_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9039aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde75fdf",
   "metadata": {},
   "source": [
    "### Q1. Does age affect home value? \n",
    "We think older homes will be least expensive and newer homes will be most expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfeae0",
   "metadata": {},
   "source": [
    "**Age Bins**\n",
    "- `our_age` < 40\n",
    "- 40 < `parents_age` < 75\n",
    "- 75 < `gma_age` < 110\n",
    "- 110 < `greatg_age` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.age_home(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ec9fe",
   "metadata": {},
   "source": [
    "# *Scripting*\n",
    "### Hypothesis Test | Older vs. Newer Home Mean Values\n",
    "We ran a `two_tailed`, `ind` hypothesis test to see if there was a statistically significant difference between the newer homes's mean value and older homes' mean value.\n",
    ">**Two tailed:** We are looking for any difference, greater or less than, between means <br>\n",
    "**Two sample:** We are comparing means across 2 samples of our data <br>\n",
    "Large enough dataset that we do not need to test for `equal_var`\n",
    "\n",
    "$Ho$: There is no statistically significant difference between the newer and older homes' mean value. <br>\n",
    "$Ha$: There is no statistically significant difference between the newer and older homes' mean value. <br>\n",
    "$α$ = .05 \n",
    "- *If $α$ < **p** value, then we fail to reject* $Ho$)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdca9a",
   "metadata": {},
   "source": [
    ">As expected, older homes had least expensive mean value and newer homes had the most expensive mean value, but there did not appear to be a significant difference in the mean home values across each age bin.\n",
    "- The two bins in the middle were almost equal, with the older bin having a slightly more expensive mean value.\n",
    "- There is a There is a statistically significant difference between the means of the youngest and oldest homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf12fb",
   "metadata": {},
   "source": [
    "### Q2 Do homes in certain locations have higher home values?\n",
    "We think we can use our features to create some clusters to create a geoplot and explore home values by location.\n",
    "##### Cluster Features\n",
    "- `beds`\n",
    "- `baths`\n",
    "- `sqft`\n",
    "\n",
    "\n",
    "***Note: It may be better to make the visualizations below on unscaled data, the axes are a little weird like in the first one some properties have a negative sq_ft value.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8108ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_validate_scaled, X_test_scaled = e.cluster_and_scale(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de43e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ce068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd3595d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54750f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67373b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929a89f2",
   "metadata": {},
   "source": [
    "## Multivariate Exploration using Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fc6c9",
   "metadata": {},
   "source": [
    "#### Are there any interesting patterns when the data is plotted geographically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ed14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_cols = ['beds', 'baths', 'sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colu in cluster_cols:\n",
    "#     sns.relplot(data = train, x = 'latitude', y = 'longitude', hue = colu, col = 'cluster')\n",
    "#     plt.title(f'{colu} Plotted Geographically')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636edc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a3159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864d48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd463f8f",
   "metadata": {},
   "source": [
    "# Exploration Takeaways \n",
    "- `age` would be a good model feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff61e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed8fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69c77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0abf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba14eda5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    ">Your conclusion summary should addresses the questions you raised in the opening of the project, which we would want to see at the end of every final notebook. Ideally, when the deliverable is a report, the summary should tie together your analysis, the drivers of the outcome, and how you would expect your ML model to perform in the future on unseen data, in layman's terms.\n",
    "\n",
    "## Recommendations\n",
    ">Your notebook should end with actionable recommendations based on your insights and analysis on a way to make a better model, such as a new feature or an algorithm or something you found that doesn't work.\n",
    "\n",
    "## Next Steps\n",
    ">Your conclusion should include next steps from a data science perspective that will assist in improving your research. Ideally, if you talk about trying more algorithms to improve performance, think about why you need to improve performance. And if the business calls for it, remember the best way to improve performance is to have better predictors/features. If you talk about gathering more data, being specific about what data you think will help you understand the problem better and why is the way to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
